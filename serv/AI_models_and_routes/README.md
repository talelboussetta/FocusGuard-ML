# FocusGuard ML Models üß†

Machine learning models for real-time focus and distraction detection during focus sessions.

## üìÅ Models

### 1. Distraction Detector (YOLOv11)
**Status:** ‚úÖ Implemented

Real-time distraction detection using YOLOv11 for person presence and phone usage monitoring.

**Features:**
- Person detection (COCO class 0)
- Cell phone detection (COCO class 67)
- Proximity analysis (phone near person detection)
- Configurable alert thresholds
- Real-time WebSocket monitoring
- Automatic distraction severity classification
- FPS monitoring and performance metrics

**How it works:**
1. Captures webcam frames from the client
2. Runs YOLOv11 object detection on each frame
3. Detects persons and cell phones in the frame
4. Calculates proximity between phone and person using IoU and distance metrics
5. Tracks phone usage duration
6. Triggers alerts when phone usage exceeds threshold (default: 10 seconds)
7. Sends real-time notifications and optionally saves events to database

**Detection Logic:**
- **Person Detection**: Confidence threshold ‚â• 0.5
- **Phone Detection**: Confidence threshold ‚â• 0.4
- **Proximity Check**: Phone within 30% of person's height or overlapping
- **Alert Trigger**: Phone usage ‚â• 10 seconds
- **Severity Levels**:
  - `LOW`: < 15 seconds phone usage
  - `MEDIUM`: 15-60 seconds phone usage
  - `HIGH`: > 60 seconds phone usage

**Usage:**

**Standalone Testing:**
```bash
# Install dependencies
pip install -r requirements.txt

# Run detector test with webcam
python -m AI_models_and_routes.distraction_detector
```

**WebSocket API Integration:**
```python
# Connect to WebSocket endpoint
ws://localhost:8000/distraction/ws/monitor?session_id={session_id}&token={jwt_token}

# Send frames (base64 encoded)
{
  "type": "frame",
  "frame": "data:image/jpeg;base64,..."
}

# Receive detection results
{
  "type": "detection",
  "data": {
    "person_detected": true,
    "phone_detected": true,
    "phone_usage_duration": 5.2,
    "should_alert": false,
    "distraction_active": false,
    "total_distractions": 0,
    "fps": 28
  },
  "annotated_frame": "data:image/jpeg;base64,..."
}

# Receive alerts
{
  "type": "alert",
  "data": {
    "alert_type": "phone_usage",
    "message": "Phone usage detected! Please focus on your work.",
    "severity": "medium",
    "duration": 12.3,
    "play_sound": true
  }
}
```

**HTTP API Endpoints:**
```bash
# Get distraction events for a session
GET /distraction/sessions/{session_id}/events

# Get distraction statistics
GET /distraction/sessions/{session_id}/stats

# Manually create a distraction event
POST /distraction/events

# Delete session distractions
DELETE /distraction/sessions/{session_id}/events
```

**Controls (Standalone Mode):**
- `q` - Quit
- `r` - Reset counters
- `s` - Save screenshot

## üöÄ Architecture

### Real-Time Monitoring Flow

```
Client (React)  ‚Üí  WebSocket  ‚Üí  FastAPI  ‚Üí  YOLO Detector
     ‚Üì                                            ‚Üì
  Webcam                                    Detection Results
     ‚Üì                                            ‚Üì
 Base64 Frame  ‚Üí  Network  ‚Üí  Decode  ‚Üí  Process  ‚Üí  Annotate
     ‚Üë                                            ‚Üì
     ‚Üê  Annotated Frame + Detection Data  ‚Üê  Encode
```

### Database Schema

**distraction_events** table:
- `id`: UUID (Primary Key)
- `session_id`: UUID (Foreign Key ‚Üí sessions)
- `user_id`: UUID (Foreign Key ‚Üí users)
- `event_type`: VARCHAR ('phone_usage', 'user_absent', 'multiple_persons')
- `duration_seconds`: INTEGER
- `severity`: VARCHAR ('low', 'medium', 'high')
- `metadata`: JSONB (detection confidence, etc.)
- `started_at`: TIMESTAMP
- `ended_at`: TIMESTAMP
- `created_at`: TIMESTAMP

## üìä Performance

**Distraction Detector (YOLOv11n):**
- Model Size: ~6 MB (Nano variant)
- Average FPS: 25-30 on CPU, 60+ on GPU
- Inference Time: ~30ms per frame (CPU), ~15ms (GPU)
- Memory Usage: ~500MB (including PyTorch)
- Accuracy: ~90% for person detection, ~75% for phone detection

**Optimization Tips:**
- Use `yolo11n.pt` (Nano) for speed on CPU
- Use `yolo11s.pt` (Small) for better accuracy on GPU
- Reduce frame rate to 15-20 FPS to save bandwidth
- Enable GPU acceleration with CUDA for 3-4x speedup

## üîß Configuration

All models support customization through initialization parameters:

```python
detector = DistractionDetector(
    model_name="yolo11n.pt",           # Model variant (n/s/m/l/x)
    person_confidence=0.5,              # Person detection threshold
    phone_confidence=0.4,               # Phone detection threshold
    phone_alert_duration=10.0,          # Seconds before alert
    proximity_threshold=0.3             # Phone-person proximity (0-1)
)
```

## üéØ Planned Improvements

### Enhanced Detection Features
- **Multi-person detection**: Alert when multiple people are in frame
- **User absence detection**: Track when user leaves the desk
- **Gaze direction tracking**: Detect if user is looking away from screen
- **Posture analysis**: Monitor sitting posture and ergonomics

### Advanced Analytics
- **Distraction patterns**: Identify common distraction times
- **Focus score calculation**: Real-time focus quality metrics
- **Productivity insights**: Correlate distractions with session completion
- **Personalized thresholds**: Adaptive alert sensitivity per user

### Model Upgrades
- **Custom YOLO training**: Fine-tune on focus-specific scenarios
- **Lightweight models**: Optimize for browser-based inference
- **Multi-modal fusion**: Combine CV with browser activity tracking

## üìñ References

**YOLOv11:**
- Ultralytics YOLOv11: https://github.com/ultralytics/ultralytics
- YOLO Documentation: https://docs.ultralytics.com/
- COCO Dataset Classes: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml

**Computer Vision:**
- OpenCV: https://opencv.org/
- PyTorch: https://pytorch.org/

## üõ°Ô∏è Privacy & Security

All models run with strong privacy guarantees:

- **Local Processing**: Frames processed in real-time on server, never stored
- **No Cloud Uploads**: All inference happens locally
- **Opt-in Monitoring**: Only active during focus sessions when user enables it
- **Data Control**: Users can delete all distraction events anytime
- **WebSocket Security**: JWT authentication required for monitoring
- **Database Encryption**: Events stored securely with user consent

**Privacy Best Practices:**
- Frames are discarded immediately after processing
- Only metadata (detection results) saved to database
- Users control when monitoring is active
- All data encrypted in transit (WSS) and at rest
- GDPR compliant with data deletion capabilities
- Complete user privacy guaranteed

## üìù License

Part of the FocusGuard ML project.
